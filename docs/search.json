[
  {
    "objectID": "canadian_births.html",
    "href": "canadian_births.html",
    "title": "Canadian Births Through Recent Years",
    "section": "",
    "text": "The original data came from this data set: here\nThis data shows how births varied in Canada throughout an individual year and over the years from 1990 to 2022. Over the 32 years, we see a dip until 2020 before it rises back up again until it peaks at 2015. It appears that the birth rate has a rising and falling pattern shifting from rising to falling every 10-15 years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "This project was developed for my Foundations of Data Science course, where I explored methods for cleaning, analyzing, and visualizing large datasets. I designed efficient data pipelines to process raw information, applied statistical techniques to uncover meaningful insights, and created clear visualizations to communicate results. The project emphasized both technical implementation and data-driven storytelling, highlighting the importance of making complex analyses accessible and interpretable."
  },
  {
    "objectID": "TheOfficeLineAnalysis.html",
    "href": "TheOfficeLineAnalysis.html",
    "title": "The Office Line-Analysis",
    "section": "",
    "text": "For this project, I chose to analyze a data set containing every line from “The Office”. The data set was found here. In this data set I wanted to look at how the characters from my favorite show appeared through the show and how an iconic joke appeared as well.\n\n\nShow the code\ntop_characters &lt;- the_office_lines |&gt;\n  filter(str_detect(Line, \"\\\\[.*\\\\]\")) |&gt;\n  group_by(Character) |&gt;\n  summarize(total_count = n(), .groups = \"drop\") |&gt;\n  slice_max(total_count, n = 10)\n\nlines_per_season &lt;- the_office_lines |&gt;\n  semi_join(top_characters, by = \"Character\") |&gt;\n  group_by(Character, Season, add = TRUE) |&gt;\n  summarize(appearences = n(), .groups = \"drop\") |&gt;\n  group_by(Season) |&gt;\n  mutate(total_season = sum(appearences)) |&gt;\n  ungroup() |&gt;\n  mutate(percent = (appearences/total_season)*100)\n\nggplot(lines_per_season, aes(x = Season, y = percent, color = Character)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  labs(\n    x = \"Season\",\n    y = \"Percentage\",\n    title = \"Lines With Direction Breakdown Per Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe first thing I analyzed is the graph above, which shows the distribution of lines each character has per season in the show where they were given stage directions, designated with [ and ] somewhere in the line. The graph displays the 10 characters with the highest total lines containing these directions in the show and their share of these lines per season. As you can see, Michael dominates the share of lines containing stage directions throughout his time on The Office, with only a slight decline until a more significant drop after season 7. Other characters saw a slight increase following this drop, but for the most part, there was an equal split among the remaining characters in terms of the percentage of lines containing stage directions.\n\n\nShow the code\nthats_what_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_lines = str_to_lower(Line)) |&gt;\n  mutate(quote = str_extract(lower_lines, \"that\\\\’?s what she said\")) |&gt;\n  filter(!is.na(quote)) |&gt;\n  mutate(season_text = str_c(Season)) |&gt;\n  group_by(season_text) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  add_row(count = 0, season_text = \"1\") |&gt;\n  add_row(count = 0, season_text = \"8\")\n\nggplot(thats_what_she_said, aes(x = season_text, y = count)) +\n  geom_col() +\n  labs(\n    x = \"Season\",\n    y = \"Quantity of Jokes\",\n    title = \"That's What She Said Jokes a Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe next thing I looked at focused on Michael’s famous joke, “that’s what she said”. The chart above breaks down the usage in the show throughout the nine seasons. As you can see in the chart, they created the joke starting in season 2 and slowly declined in use as the show went on before Michael’s departure following season season 7. There is a return in season 9, likely due to Michael’s return for the finale.\n\n\nShow the code\nwords_preceding_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_case = str_to_lower(Line)) |&gt;\n  mutate(char_following = str_extract(lower_case, \"(?&lt;=that’s what she said).\")) |&gt;\n  filter(!is.na(char_following)) |&gt;\n  group_by(char_following) |&gt;\n  summarize(count = n())\n  \nggplot(words_preceding_she_said, aes(x = char_following, y = count, text = char_following)) +\n  geom_col() +\n  labs(\n    x = \"Character following\",\n    y = \"Appearences\",\n    title = 'Character following \"Thats what she said\"'\n  ) +\n  theme(\n    axis.text.x = element_text(size = 20, face = \"bold\"),\n    axis.text.y = element_text(size = 14),\n    axis.title.x = element_text(size = 16),\n    axis.title.y = element_text(size = 16),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nFinally I was curious how this joke was typically written into the show. The chart above does this by breaking down which punctuation was used following the joke. This is able to tell us how the joke was presented. For example, we are able to figure out that more often than not, the joke is not written as Michael screaming out “That’s what she said!”, rather he delivers it less excited. You are also able to see that it is written in other ways as well such as the ’, which is a line where Jim quotes the joke inside of a question. What we see is they often didn’t direct the joke to be delivered with a lot of excitement, rather they left it up to the actor who was making the joke to decide how to deliver it."
  },
  {
    "objectID": "Game_Simulations.html",
    "href": "Game_Simulations.html",
    "title": "Basketball Game Simulations",
    "section": "",
    "text": "For this project I amied to simulate basketball games based on both the teams strength and on the opposing teams strength. First I will build a function that can simulate a basketball game based on two inputs, the strength of the focus team and the strength of the opposing team. To create variability we will make opposing strength more a random strength value centered around the input. Next we will simulate this many times to get a season percentage. We will then repeat the season percentage for each pairing of strengths 1-10 many times to get the statistics.\nThe following code is the simulation done for each game. Each game is composed by fitting a score for each team on a normal curve bsed on their strength then comparing to see who won. Additionally I added an if statement in there incase they get the same score. In basketball they never have a tied game so I had to ensure that for accuracy they couldn’t tie int he simulation either. Here is an example output:\n\n\nShow the code\nsimulate_game &lt;- function(count, strength, opposing_strength_avg) {\n  opposing_strength &lt;- rnorm(1, opposing_strength_avg, 1)\n  \n  main_team_score = round(rnorm(1, mean = 115 + strength - opposing_strength, abs(1-strength)))\n  away_team_score = round(rnorm(1, mean = 115 + opposing_strength - strength, abs(1-strength)))\n  difference = main_team_score - away_team_score\n  if (difference == 0) {\n    if (strength &gt; opposing_strength) {\n      main_team_score &lt;- main_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    } else if (strength &lt; opposing_strength) {\n      away_team_score &lt;- away_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    }\n  }\n  \n  results &lt;- data.frame(\n    game = count,\n    main_team_score = main_team_score,\n    away_team_score = away_team_score,\n    strength = strength,\n    opposing_strength = opposing_strength,\n    difference = difference\n  )\n  return (results)\n}\n\nsimulate_game(1, 5, 5)\n\n\n  game main_team_score away_team_score strength opposing_strength difference\n1    1             116             118        5          6.994696         -2\n\n\nThis next code chunk simulates the previous game simulation 82 times, giving a season percentage. Here is an example output for that code:\n\n\nShow the code\nseason_simulation &lt;- function(count, strength_input,  opposing_strength_avg_input) {\n  params &lt;- list(\n    count = 1:82,\n    strength = rep(strength_input, 82),\n    opposing_strength_avg = rep(opposing_strength_avg_input, 82)\n  )\n\n  season &lt;- list_rbind(pmap(params, simulate_game)) |&gt;\n    select(difference) |&gt;\n    mutate(win = difference &gt; 0)\n  return_df &lt;- data.frame(\n    season_perc = mean(season$win)\n  )\n  return (return_df)\n}\n\nseason_simulation(1, 5, 5)\n\n\n  season_perc\n1   0.5365854\n\n\nNext this code runs the previous simulation 50 times giving us 50 seasons of 1 strength input combination allowing us to use Central Limit Theorem on the outputted values and giving us reliable data:\n\n\nShow the code\nperc_sim &lt;- function(strength, opposing_strength) {\n  params &lt;- list(\n      count = 1:50,\n      strength_input = rep(strength, 50),\n      opposing_strength_avg_input = rep(opposing_strength, 50)\n    )\n  itterations &lt;- list_rbind(pmap(params, season_simulation)) |&gt;\n    mutate(strength_level = strength) |&gt;\n    mutate(opposing_strength = opposing_strength) |&gt;\n    mutate(winning_perc = mean(season_perc))\n  return(\n    itterations |&gt;\n      summarise(\n        winning_perc = mean(winning_perc),\n        strength_level = first(strength_level),\n        opposing_strength = first(opposing_strength)\n      )\n  )\n}\n\nperc_sim(5, 5)\n\n\n  winning_perc strength_level opposing_strength\n1    0.5004878              5                 5\n\n\nFinally to finish off our simulation I am going to run the previous code chunk for each combination of code giving us a data frame that has an average percentage for each combination of strength inputs.\n\n\nShow the code\nsim_params &lt;- list(\n  strength = rep(1:10, times = 10),\n  opposing_strength = rep(1:10, each = 10)\n)\n\nperc_breakdown &lt;- list_rbind(pmap(sim_params, perc_sim))\n\n\nNow to visualize the data, the first graph is a heat map with the focus teams strength on the x-axis and the opposing strength team on the y-axis. This gives us a good showing of how the matchups go. Interestingly, there is a slight advantage to the focus team when they are roughly equal but both have high input values compared to low.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level, y = opposing_strength, fill = winning_perc)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"red\", high = \"green\", name = \"Win %\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe other visual representation we have is a scatter plot that has strength - opposing strength on the x-axis and win percentage on the y-axis. This shows an interesting layout of how fast the differences in strength can affect the results of the games.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level - opposing_strength, y = winning_perc)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThese graphs show great insight into just how important it is to have a strong team relative to the rest of the league. Despite the simple simulation for each game, we find that just by having a somewhat equal team relative to the rest of the league you will make the playoffs, the average wins of the worst seed is around 50%. We also see that it only takes around a 2 or 3 strength index advantage to be able to get a very strong season put together. That being said this shows just how even the teams are in strength. The next steps would be to use real data and team analysis to be able to pinpoint just what the strngth index would be based off of but this is a very interesting and good start to this type of analysis."
  },
  {
    "objectID": "Attendance.html",
    "href": "Attendance.html",
    "title": "Packers VS Viking Attendance",
    "section": "",
    "text": "The original data came from this data set: here\nThis data compares the attendence of the Green Bay packers and the Minnesota Vikings throughout the seasons of 2018 and 2019. Based on the graphs, it appears that on average the Packers typically have a higher attendence during a game compared to the Vikings. There are fluctuations however due to things such as home and aways games and success that season. For example the Packers had higher success in the 2019 season than in 2018 so their attendence was higher throught the year."
  },
  {
    "objectID": "Project 4.html",
    "href": "Project 4.html",
    "title": "Review of Amazon AI Ethics",
    "section": "",
    "text": "Both of the articles that I selected dove into issues surrounding advancements in AI and its use around hiring. The scenario these articles focus on is how an AI developed by Amazon, used to recommend job candidates, had to be scrapped based on a bias found present where the AI would favor recommending men and penalized applicants whose resume contained the word woman such as woman’s chess club (Dastin). This is due to the data that was collected by Amazon, which contained an inherent bias already. These articles both layed out those facts very similarly. Where they differed was how this issue arose. Reuters argued that the blame was not on Amazon and they handled the situation as best as they could. They argued that it was difficult to see a bias like this coming and that it would be difficult to predict this outcome. The ACLU on the other hand argued something different. They claimed that it is in fact on Amazon. They stated that it is a structural injustice embedded in technology and that it is up to the creators of technologies, such as the AI amazon made, to ensure that these biases don’t appear. Where Reuters’ argument fell behind was the fact that Amazon was unable to deny the fact that they had used the AI in consideration before it was scrapped. That means this bias was being used, something that is illegal, though hard to prove in the court of law (Goodman).\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations? Amazon’s data collection process was entirely internal, drawing on its own hiring history to train the AI. The company argued that using this data would help identify applicants who shared traits—such as mindset or work ethic—with previously successful hires. However, because the tech industry has historically been male-dominated, the data itself was skewed. As a result, the AI began to favor male candidates and penalize resumes associated with women. This outcome highlights a key ethical concern: the observations used for training were inherently biased, and relying on them reinforced discriminatory patterns. Since the data was proprietary and not publicly released, it’s also unclear whether there were missing or unbalanced observations that further affected the algorithm’s performance.\nWere the data made publicly available? The data used in Amazon’s hiring algorithm was not publicly available, as it came from internal recruitment records. However, it is known that the data reflected a predominantly male applicant pool, which contributed to the gender bias observed in the AI’s recommendations. Both articles emphasized that when hiring processes are\nIs the data being used in unintended ways to the original study? In this scenario, Amazon intended to use data and AI to streamline the hiring process and reduce the effort spent on early-stage recruitment. While the system achieved that goal technically, it did so by learning from biased historical data. Because past hiring patterns reflected gender bias, the AI began to penalize resumes containing words like “women,” leading to discriminatory outcomes. Although the original intent was efficiency, the AI ended up replicating and amplifying bias, particularly against women. What makes this especially concerning is that Amazon actively used the AI in its hiring process, meaning it likely influenced real decisions and may have increased gender bias in hiring rather than reducing it.\nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender? Race should not be used as a variable in algorithms. When race (or gender) is included as a variable, any preexisting human biases embedded in historical data will be learned and amplified by the algorithm. Additionally, it will just increase the rate that these biases are done. For example, in a hiring scenario, if past decisions reflect gender discrimination—such as underrepresentation of women in tech roles—an AI trained on that data may learn to associate being a woman with lower hiring potential, resulting in discriminatory outcomes. Similarly, in criminal justice systems, including race as a variable can cause the algorithm to replicate and reinforce past racial biases, such as those present in arrest or sentencing records. In both cases, using sensitive identity markers like race or gender risks perpetuating structural discrimination rather than correcting it.\nCitations:\nAmazon hiring algorithm (systemic bias and accountability) Goodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women,” ACLU.\nAmazon hiring algorithm (who has moral responsibility) Dastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women, Reuters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SQL Connection.html",
    "href": "SQL Connection.html",
    "title": "SQL Connection",
    "section": "",
    "text": "library(DBI)\nlibrary(dplyr)\nlibrary(tidyverse)\n\nFor this project we were given access to the database that contains traffic stops throughout the US. As someone that grew up in Oregon and now goes to school in California, I was curious about the three west coast states. I wanted to look at the differences in racial breakdowns of the stops between the three states. Furthermore I was curious how this breakdown compares to the true population breakdown of these states.\nFirst we connected to the server using code that is not shown so I don’t show the password.\nNext we wanted to look at the breakdowns of the traffic stops depending on race. To look at this we looked at each states traffic stop data and combined the data together into percentages of the states traffic stops. This then allowed me to get a table with all three states and their ethnic breakdown.\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\n\nwest_coast_sql\n\n             subject_race count_race percentage      state\n1                   white   14077645   44.29925 California\n2                hispanic   10512845   33.08161 California\n3                   black    2617665    8.23722 California\n4                   other    2390504    7.52239 California\n5  asian/pacific islander    2179855    6.85952 California\n6                 unknown          1    0.00000 California\n7                   white     983532   86.04701     Oregon\n8                hispanic     105491    9.22917     Oregon\n9  asian/pacific islander      24089    2.10749     Oregon\n10                  black      20957    1.83348     Oregon\n11                  other       5080    0.44444     Oregon\n12                unknown       3868    0.33840     Oregon\n13                  white    6287746   55.47966 Washington\n14                unknown    3193337   28.17628 Washington\n15               hispanic     794563    7.01079 Washington\n16 asian/pacific islander     511806    4.51590 Washington\n17                  black     403349    3.55893 Washington\n18                  other     142624    1.25844 Washington\n\n\nNext I graphed all three of the states in a side by side plot to compare the differences directly. What we found was that Oregon had far and away the most people that were considered white at the traffic stops. While California had the lowest number of white people, we saw they had the highest hispanic make-up, something to be expected as their state boarders Mexico. The most interesting finding from this was that a high quantity of the stops were unkown in Washington. This leads me to believe that they either arn’t able to ask their race/ethnicity when they make a stop or they struggle to ask consistently.\n\nggplot(west_coast_sql, aes(x = subject_race, y = percentage, fill = state)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Percentage of Each Race by State\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = \"State\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext I was curious not about the differences between the states but the differences between the traffic stop breakdown and the true population makeup of these states. To figure this out I found data from the census that I was able to build into the table that we originally made. This allowed me to set up a chart where we could compare the differences.\n\n-- Static population data\nSELECT \n  subject_race, \n  NULL AS count_race, \n  percentage, \n  state\nFROM (\n  SELECT 'white' AS subject_race, 71.0 AS percentage, 'Oregon Population' AS state\n  UNION ALL SELECT 'hispanic', 15.16, 'Oregon Population'\n  UNION ALL SELECT 'asian', 4.4, 'Oregon Population'\n  UNION ALL SELECT 'black', 1.8, 'Oregon Population'\n  UNION ALL SELECT 'native american', 1.0, 'Oregon Population'\n  UNION ALL SELECT 'pacific islander', 0.4, 'Oregon Population'\n  UNION ALL SELECT 'two or more races', 6.2, 'Oregon Population'\n\n  UNION ALL SELECT 'white', 63.0, 'Washington Population'\n  UNION ALL SELECT 'hispanic', 14.6, 'Washington Population'\n  UNION ALL SELECT 'asian', 9.3, 'Washington Population'\n  UNION ALL SELECT 'black', 3.8, 'Washington Population'\n  UNION ALL SELECT 'native american', 1.1, 'Washington Population'\n  UNION ALL SELECT 'pacific islander', 0.7, 'Washington Population'\n  UNION ALL SELECT 'two or more races', 7.5, 'Washington Population'\n\n  UNION ALL SELECT 'white', 35.0, 'California Population'\n  UNION ALL SELECT 'hispanic', 39.4, 'California Population'\n  UNION ALL SELECT 'asian', 15.0, 'California Population'\n  UNION ALL SELECT 'black', 5.0, 'California Population'\n  UNION ALL SELECT 'native american', 0.8, 'California Population'\n  UNION ALL SELECT 'pacific islander', 0.3, 'California Population'\n  UNION ALL SELECT 'two or more races', 4.5, 'California Population'\n) AS static_data\n\nUNION ALL\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\nOnce we had the table built, we compared them by faceting them side by side with the traffic stops on the left and the true populations on the right. This allowed for easy comparison. What we found in California was that more of the people involved in traffic stops were white than in the true population. The difference showed up in the hipanic part of the graph with less hispanic stopping compared to the true population. This could be because the true population is self reported and the police making the stop struggling to know their true ethnicity. We saw a similar thing occur in Oregon as well. The differences there were probably due to the self reporting once again. The only state that had an increase in white from traffic stops to the actual population was Washington. Going against the other two, this leads us to question if it is due to the high number of unknown or if it is due to a way the traffic stops are being made.\n\nggplot(combined_data, aes(x = subject_race, y = percentage, fill = subject_race)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ state, nrow = 3, ncol = 2) +\n  labs(\n    title = \"Original Racial Composition by State (from SQL Data)\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 12), \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14)\n  ) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nOverall I wanted to look at the three states and the way race is compared internally and within the three states. What I found was that there was poor data collection when the traffic stops were made but there is a high likely hood that these stops are not made with an underlying bias if the traffic stops were made with the errors there appears to be. That being said there needs to be further investigation into the way data is collected to make sure that we are able to get accurate data that reflects what is put on the census forms.\n\nDBI::dbDisconnect(con_traffic)\n\nRefrences\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nU.S. Census Bureau. 2023. QuickFacts: California; Oregon; Washington. U.S. Department of Commerce. Retrieved from https://www.census.gov/quickfacts/"
  },
  {
    "objectID": "SQL_Connection.html",
    "href": "SQL_Connection.html",
    "title": "SQL Connection",
    "section": "",
    "text": "library(DBI)\nlibrary(dplyr)\nlibrary(tidyverse)\n\nFor this project, we were given access to a database containing traffic stop data from across the United States. As someone who grew up in Oregon and now attends college in California, I was particularly interested in the three West Coast states—Oregon, California, and Washington. I wanted to examine the racial breakdown of traffic stops in each state and compare those figures to the actual population demographics. This comparison could reveal potential disparities in how different racial groups are stopped by law enforcement.\nTo begin the analysis, we connected to the database using secure credentials (not shown here to protect access).\nNext, we analyzed the racial breakdown of traffic stops in each state. To do this, we examined the traffic stop data for Oregon, California, and Washington individually, then calculated the percentage of total stops attributed to each racial group within each state. This approach allowed us to standardize the data and make meaningful comparisons across states. The resulting table displays the racial composition of traffic stops side by side for all three states.\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\n\nwest_coast_sql\n\n             subject_race count_race percentage      state\n1                   white   14077645   44.29925 California\n2                hispanic   10512845   33.08161 California\n3                   black    2617665    8.23722 California\n4                   other    2390504    7.52239 California\n5  asian/pacific islander    2179855    6.85952 California\n6                 unknown          1    0.00000 California\n7                   white     983532   86.04701     Oregon\n8                hispanic     105491    9.22917     Oregon\n9  asian/pacific islander      24089    2.10749     Oregon\n10                  black      20957    1.83348     Oregon\n11                  other       5080    0.44444     Oregon\n12                unknown       3868    0.33840     Oregon\n13                  white    6287746   55.47966 Washington\n14                unknown    3193337   28.17628 Washington\n15               hispanic     794563    7.01079 Washington\n16 asian/pacific islander     511806    4.51590 Washington\n17                  black     403349    3.55893 Washington\n18                  other     142624    1.25844 Washington\n\n\nI then created a side-by-side bar plot to visually compare the racial breakdown of traffic stops across the three states. The results showed that Oregon had the highest proportion of individuals identified as white in its traffic stop data. In contrast, California had the lowest proportion of white individuals and the highest proportion of Hispanic individuals—an expected trend given its large Hispanic population and proximity to the Mexican border. One particularly notable finding was the high percentage of traffic stops labeled as “unknown” in Washington. This suggests that either law enforcement officers in Washington are not required to record race/ethnicity during stops, or that there may be inconsistencies in how that information is collected.\n\nggplot(west_coast_sql, aes(x = subject_race, y = percentage, fill = state)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Percentage of Each Race by State\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = \"State\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext, I shifted focus from comparing the three states to examining the gap between the racial breakdown of traffic stops and the actual population demographics within each state. To do this, I incorporated U.S. Census data into the original table, aligning each racial group’s share of traffic stops with their corresponding share of the state’s population. This allowed me to create a comparative chart that highlights potential disparities between who is being stopped and the overall demographic makeup of each state.\n\n-- Static population data\nSELECT \n  subject_race, \n  NULL AS count_race, \n  percentage, \n  state\nFROM (\n  SELECT 'white' AS subject_race, 71.0 AS percentage, 'Oregon Population' AS state\n  UNION ALL SELECT 'hispanic', 15.16, 'Oregon Population'\n  UNION ALL SELECT 'asian', 4.4, 'Oregon Population'\n  UNION ALL SELECT 'black', 1.8, 'Oregon Population'\n  UNION ALL SELECT 'native american', 1.0, 'Oregon Population'\n  UNION ALL SELECT 'pacific islander', 0.4, 'Oregon Population'\n  UNION ALL SELECT 'two or more races', 6.2, 'Oregon Population'\n\n  UNION ALL SELECT 'white', 63.0, 'Washington Population'\n  UNION ALL SELECT 'hispanic', 14.6, 'Washington Population'\n  UNION ALL SELECT 'asian', 9.3, 'Washington Population'\n  UNION ALL SELECT 'black', 3.8, 'Washington Population'\n  UNION ALL SELECT 'native american', 1.1, 'Washington Population'\n  UNION ALL SELECT 'pacific islander', 0.7, 'Washington Population'\n  UNION ALL SELECT 'two or more races', 7.5, 'Washington Population'\n\n  UNION ALL SELECT 'white', 35.0, 'California Population'\n  UNION ALL SELECT 'hispanic', 39.4, 'California Population'\n  UNION ALL SELECT 'asian', 15.0, 'California Population'\n  UNION ALL SELECT 'black', 5.0, 'California Population'\n  UNION ALL SELECT 'native american', 0.8, 'California Population'\n  UNION ALL SELECT 'pacific islander', 0.3, 'California Population'\n  UNION ALL SELECT 'two or more races', 4.5, 'California Population'\n) AS static_data\n\nUNION ALL\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\nOnce the table was complete, we visualized the data using a faceted chart that placed traffic stop data on the left and population demographics on the right for each state. This layout allowed for easy side-by-side comparison. In California, we observed that white individuals made up a higher proportion of traffic stops than their share of the general population. The difference was most noticeable in the Hispanic category, where fewer Hispanic individuals were stopped relative to their proportion of the population. One possible explanation is that census data is self-reported, while officers may rely on visual assessments during traffic stops, potentially leading to underreporting or misclassification of ethnicity. A similar pattern appeared in Oregon, where discrepancies may also stem from differences in reporting methods. Interestingly, Washington was the only state where the percentage of white individuals was higher in the general population than in traffic stops. This reversal raises questions—either the large number of “unknown” entries in Washington’s stop data is skewing the results, or there may be a difference in how traffic stops are being conducted or recorded in that state.\n\nggplot(combined_data, aes(x = subject_race, y = percentage, fill = subject_race)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ state, nrow = 3, ncol = 2) +\n  labs(\n    title = \"Original Racial Composition by State (from SQL Data)\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 12), \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14)\n  ) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nOverall, my goal was to examine how race is represented within traffic stop data both within and across the three West Coast states. The analysis revealed inconsistencies and limitations in the way racial data is collected during traffic stops, suggesting that the dataset may not be fully reliable. While some discrepancies between traffic stop data and census demographics could indicate potential bias, it is also possible that these gaps are due to poor or inconsistent data collection practices rather than intentional discrimination. This highlights the need for improved and standardized data collection procedures, so that future analyses can more accurately reflect demographic realities and support informed policy decisions.\n\nDBI::dbDisconnect(con_traffic)\n\nRefrences\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nU.S. Census Bureau. 2023. QuickFacts: California; Oregon; Washington. U.S. Department of Commerce. Retrieved from https://www.census.gov/quickfacts/"
  },
  {
    "objectID": "SlideShow.html#text",
    "href": "SlideShow.html#text",
    "title": "title of presentation",
    "section": "Text",
    "text": "Text\n\n#code"
  },
  {
    "objectID": "SlideShow.html#background",
    "href": "SlideShow.html#background",
    "title": "Journey Through the Simulation",
    "section": "Background",
    "text": "Background\nFocused on sports\nFigured a team sport would be the easiest to recreate\nTurned to Basketball due to the high scores and lower variation"
  },
  {
    "objectID": "SlideShow.html#the-first-game",
    "href": "SlideShow.html#the-first-game",
    "title": "Journey Through the Simulation",
    "section": "The First Game",
    "text": "The First Game\nBroke down a game to purely the score\nUtilized normal curves for variation and realism\nThe first step was recreating a game"
  },
  {
    "objectID": "SlideShow.html#the-code-and-the-output",
    "href": "SlideShow.html#the-code-and-the-output",
    "title": "Journey Through the Simulation",
    "section": "The Code and the Output",
    "text": "The Code and the Output\n\nsimulate_game &lt;- function(count, strength, opposing_strength_avg) {\n  opposing_strength &lt;- rnorm(1, opposing_strength_avg, 1)\n  \n  main_team_score = round(rnorm(1, mean = 115 + strength - opposing_strength, abs(1-strength)))\n  away_team_score = round(rnorm(1, mean = 115 + opposing_strength - strength, abs(1-strength)))\n  difference = main_team_score - away_team_score\n  if (difference == 0) {\n    if (strength &gt; opposing_strength) {\n      main_team_score &lt;- main_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    } else if (strength &lt; opposing_strength) {\n      away_team_score &lt;- away_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    }\n  }\n  \n  results &lt;- data.frame(\n    game = count,\n    main_team_score = main_team_score,\n    away_team_score = away_team_score,\n    strength = strength,\n    opposing_strength = opposing_strength,\n    difference = difference\n  )\n  return (results)\n}\n\nsimulate_game(1, 5, 5)\n\n  game main_team_score away_team_score strength opposing_strength difference\n1    1             112             111        5          4.453558          1"
  },
  {
    "objectID": "SlideShow.html#the-code",
    "href": "SlideShow.html#the-code",
    "title": "Journey Through the Simulation",
    "section": "The Code",
    "text": "The Code\n\nCreating a GameCreating a SeasonSimulating 50 TimesAll Matchups\n\n\n\nsimulate_game &lt;- function(count, strength, opposing_strength_avg) {\n  opposing_strength &lt;- rnorm(1, opposing_strength_avg, 1)\n  \n  main_team_score = round(rnorm(1, mean = 115 + strength - opposing_strength, abs(1-strength)))\n  away_team_score = round(rnorm(1, mean = 115 + opposing_strength - strength, abs(1-strength)))\n  difference = main_team_score - away_team_score\n  if (difference == 0) {\n    if (strength &gt; opposing_strength) {\n      main_team_score &lt;- main_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    } else if (strength &lt; opposing_strength) {\n      away_team_score &lt;- away_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    }\n  }\n  \n  results &lt;- data.frame(\n    game = count,\n    main_team_score = main_team_score,\n    away_team_score = away_team_score,\n    strength = strength,\n    opposing_strength = opposing_strength,\n    difference = difference\n  )\n  return (results)\n}\n\nsimulate_game(1, 5, 5)\n\n  game main_team_score away_team_score strength opposing_strength difference\n1    1             116             118        5          6.994696         -2\n\n\n\n\n\nseason_simulation &lt;- function(count, strength_input,  opposing_strength_avg_input) {\n  params &lt;- list(\n    count = 1:82,\n    strength = rep(strength_input, 82),\n    opposing_strength_avg = rep(opposing_strength_avg_input, 82)\n  )\n\n  season &lt;- list_rbind(pmap(params, simulate_game)) |&gt;\n    select(difference) |&gt;\n    mutate(win = difference &gt; 0)\n  return_df &lt;- data.frame(\n    season_perc = mean(season$win)\n  )\n  return (return_df)\n}\n\nseason_simulation(1, 5, 5)\n\n  season_perc\n1   0.5365854\n\n\n\n\n\nperc_sim &lt;- function(strength, opposing_strength) {\n  params &lt;- list(\n      count = 1:50,\n      strength_input = rep(strength, 50),\n      opposing_strength_avg_input = rep(opposing_strength, 50)\n    )\n  itterations &lt;- list_rbind(pmap(params, season_simulation)) |&gt;\n    mutate(strength_level = strength) |&gt;\n    mutate(opposing_strength = opposing_strength) |&gt;\n    mutate(winning_perc = mean(season_perc))\n  return(\n    itterations |&gt;\n      summarise(\n        winning_perc = mean(winning_perc),\n        strength_level = first(strength_level),\n        opposing_strength = first(opposing_strength)\n      )\n  )\n}\n\nperc_sim(5, 5)\n\n  winning_perc strength_level opposing_strength\n1    0.5004878              5                 5\n\n\n\n\n\n\n   winning_perc strength_level opposing_strength\n1     0.5046341              1                 1\n2     0.8095122              2                 1\n3     0.8953659              3                 1\n4     0.9160976              4                 1\n5     0.9200000              5                 1\n6     0.9200000              6                 1\n7     0.9287805              7                 1\n8     0.9219512              8                 1\n9     0.9243902              9                 1\n10    0.9229268             10                 1"
  },
  {
    "objectID": "SlideShow.html#the-results",
    "href": "SlideShow.html#the-results",
    "title": "Journey Through the Simulation",
    "section": "The Results",
    "text": "The Results\n\nHeat Map breakdownScatter Plot Breakdown"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"Resume - Henry Cannon-2.pdf\"&gt;Download PDF&lt;/a&gt;."
  }
]