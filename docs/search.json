[
  {
    "objectID": "canadian_births.html",
    "href": "canadian_births.html",
    "title": "Canadian Births",
    "section": "",
    "text": "The original data came from this data set: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-01-09/canada_births_1991_2022.csv\nThis data shows how births varied in Canada throughout an individual year and over the years from 1990 to 2022. Over the 32 years, we see a dip until 2020 before it rises back up again until it peaks at 2015. It appears that the birth rate has a rising and falling pattern shifting from rising to falling every 10-15 years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I am a student at Pomona College. I swim on the swim team and plan to major in either Computer Science, or Math. In my free time I like to hang out with friends, kiteboard/wingfoil, and play games."
  },
  {
    "objectID": "TheOfficeLineAnalysis.html",
    "href": "TheOfficeLineAnalysis.html",
    "title": "The Office Line-Analysis",
    "section": "",
    "text": "For this project, I chose to analyze a data set containing every line from the she “The Office”. The data set was found at https://www.kaggle.com/datasets/fabriziocominetti/the-office-lines?resource=download. In this data set I wanted to look at how the characters from my favorite show appeared through the show and how an iconic joke appeared as well.\n\n\nShow the code\ntop_characters &lt;- the_office_lines |&gt;\n  group_by(Character) |&gt;\n  summarize(total_count = n(), .groups = \"drop\") |&gt;\n  slice_max(total_count, n = 10)\n\nlines_per_season &lt;- the_office_lines |&gt;\n  filter(Character %in% top_characters$Character) |&gt;\n  group_by(Character, Season, add = TRUE) |&gt;\n  summarize(appearences = n(), .groups = \"drop\") |&gt;\n  group_by(Season) |&gt;\n  mutate(total_season = sum(appearences)) |&gt;\n  ungroup() |&gt;\n  mutate(percent = (appearences/total_season)*100)\n\nggplot(lines_per_season, aes(x = Season, y = percent, color = Character)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  labs(\n    x = \"Season\",\n    y = \"Percentage\",\n    title = \"Line Breakdown Per Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe first thing that I analyzed is in the graph above, showing the distribution of lines characters have each season of the show. The graph shows the 10 characters with the highest total lines in the show and their share of each season of the office between the 10. As you can see Micheal dominates the share of lines through his time with the office, only slightly declining throughout. As he leaves the show following season 8, you see a jump in other characters roles as they needed to fill the gap that Michael left, such as Andy and Dwight. This graph shows the importance that Michael had to the show and how much it changed in its last two seasons.\n\n\nShow the code\nthats_what_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_lines = str_to_lower(Line)) |&gt;\n  mutate(quote = str_extract(lower_lines, \"that’s what she said\")) |&gt;\n  filter(!is.na(quote)) |&gt;\n  mutate(season_text = str_c(Season)) |&gt;\n  group_by(season_text) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  add_row(count = 0, season_text = \"1\") |&gt;\n  add_row(count = 0, season_text = \"8\")\n\nggplot(thats_what_she_said, aes(x = season_text, y = count)) +\n  geom_col() +\n  labs(\n    x = \"Season\",\n    y = \"Quantity of Jokes\",\n    title = \"That's What She Said Jokes a Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe next thing I looked at focused on Michaels famous joke, “that’s what she said”. The chart above breaks down the usage in the show throughout the nine seasons. As you can see in the chart, they created the joke starting in season 2 and slowly declined in use as the show went on before Michael’s departure following season season 7. There is a return in season 9, likely due to Michael’s return for the finale.\n\n\nShow the code\nwords_preceding_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_case = str_to_lower(Line)) |&gt;\n  mutate(char_following = str_extract(lower_case, \"(?&lt;=that’s what she said).\")) |&gt;\n  filter(!is.na(char_following)) |&gt;\n  group_by(char_following) |&gt;\n  summarize(count = n())\n  \nggplot(words_preceding_she_said, aes(x = char_following, y = count, text = char_following)) +\n  geom_col() +\n  labs(\n    x = \"Character following\",\n    y = \"Appearences\",\n    title = 'Character following \"Thats what she said\"'\n  ) +\n  theme(\n    axis.text.x = element_text(size = 20, face = \"bold\"),\n    axis.text.y = element_text(size = 14),\n    axis.title.x = element_text(size = 16),\n    axis.title.y = element_text(size = 16),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nFinally I was curious how this joke was typically written into the show. The chart above does this by breaking down which punctuation was used following the joke. This is able to tell us how the joke was presented. For example, we are able to figure out that more often than not, the joke is not written as Michael screaming out “That’s what she said!”, rather he delivers it less excited. You are also able to see that it is written in other ways as well such as the ’, which is a line where Jim quotes the joke inside of a question. What we see is they often didn’t direct the joke to be delivered with a lot of excitement, rather they left it up to the actor who was making the joke to decide how to deliver it."
  },
  {
    "objectID": "Game_Simulations.html",
    "href": "Game_Simulations.html",
    "title": "Basketball Game Simulations",
    "section": "",
    "text": "For this project I amied to simulate basketball games based on both the teams strength and on the opposing teams strength. First I will build a function that can simulate a basketball game based on two inputs, the strength of the focus team and the strength of the opposing team. To create variability we will make opposing strength more a random strength value centered around the input. Next we will simulate this many times to get a season percentage. We will then repeat the season percentage for each pairing of strengths 1-10 many times to get the statistics.\nThe following code is the simulation done for each game. Here is an example output:\n\n\nShow the code\nsimulate_game &lt;- function(count, strength, opposing_strength_avg) {\n  opposing_strength &lt;- rnorm(1, opposing_strength_avg, 1)\n  \n  main_team_score = round(rnorm(1, mean = 115 + strength - opposing_strength, abs(1-strength)))\n  away_team_score = round(rnorm(1, mean = 115 + opposing_strength - strength, abs(1-strength)))\n  difference = main_team_score - away_team_score\n  if (difference == 0) {\n    if (strength &gt; opposing_strength) {\n      main_team_score &lt;- main_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    } else if (strength &lt; opposing_strength) {\n      away_team_score &lt;- away_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    }\n  }\n  \n  results &lt;- data.frame(\n    game = count,\n    main_team_score = main_team_score,\n    away_team_score = away_team_score,\n    strength = strength,\n    opposing_strength = opposing_strength,\n    difference = difference\n  )\n  return (results)\n}\n\nsimulate_game(1, 5, 5)\n\n\n  game main_team_score away_team_score strength opposing_strength difference\n1    1             116             118        5          6.994696         -2\n\n\nThis next code chunk simulates the previous game simulation 82 times, giving a season percentage. Here is an example output for that code:\n\n\nShow the code\nseason_simulation &lt;- function(count, strength_input,  opposing_strength_avg_input) {\n  params &lt;- list(\n    count = 1:82,\n    strength = rep(strength_input, 82),\n    opposing_strength_avg = rep(opposing_strength_avg_input, 82)\n  )\n\n  season &lt;- list_rbind(pmap(params, simulate_game)) |&gt;\n    select(difference) |&gt;\n    mutate(win = difference &gt; 0)\n  return_df &lt;- data.frame(\n    season_perc = mean(season$win)\n  )\n  return (return_df)\n}\n\nseason_simulation(1, 5, 5)\n\n\n  season_perc\n1   0.5365854\n\n\nNext this code runs the previous simulation 50 times giving us 50 seasons of 1 strength input combination allowing us to use Central Limit Theorem on the outputted values and giving us reliable data:\n\n\nShow the code\nperc_sim &lt;- function(strength, opposing_strength) {\n  params &lt;- list(\n      count = 1:50,\n      strength_input = rep(strength, 50),\n      opposing_strength_avg_input = rep(opposing_strength, 50)\n    )\n  itterations &lt;- list_rbind(pmap(params, season_simulation)) |&gt;\n    mutate(strength_level = strength) |&gt;\n    mutate(opposing_strength = opposing_strength) |&gt;\n    mutate(winning_perc = mean(season_perc))\n  return(\n    itterations |&gt;\n      summarise(\n        winning_perc = mean(winning_perc),\n        strength_level = first(strength_level),\n        opposing_strength = first(opposing_strength)\n      )\n  )\n}\n\nperc_sim(5, 5)\n\n\n  winning_perc strength_level opposing_strength\n1    0.5004878              5                 5\n\n\nFinally to finish off our simulation I am going to run the previous code chunk for each combination of code giving us a data frame that has an average percentage for each combination of strength inputs.\n\n\nShow the code\nsim_params &lt;- list(\n  strength = rep(1:10, times = 10),\n  opposing_strength = rep(1:10, each = 10)\n)\n\nperc_breakdown &lt;- list_rbind(pmap(sim_params, perc_sim))\n\n\nNow to visualize the data, the first graph is a heat map with the focus teams strength on the x-axis and the opposing strength team on the y-axis. This gives us a good showing of how the matchups go. Interestingly, there is a slight advantage to the focus team when they are roughly equal but both have high input values compared to low.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level, y = opposing_strength, fill = winning_perc)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"red\", high = \"green\", name = \"Win %\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe other visual representation we have is a scatter plot that has strength - opposing strength on the x-axis and win percentage on the y-axis. This shows an interesting layout of how fast the differences in strength can affect the results of the games.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level - opposing_strength, y = winning_perc)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThese graphs show great insight into just how important it is to have a strong team relative to the rest of the league. Despite the simple simulation for each game, we find that just by having a somewhat equal team relative to the rest of the league you will make the playoffs, the average wins of the worst seed is around 50%. We also see that it only takes around a 2 or 3 strength index advantage to be able to get a very strong season put together. That being said this shows just how even the teams are in strength. The next steps would be to use real data and team analysis to be able to pinpoint just what the strngth index would be based off of but this is a very interesting and good start to this type of analysis."
  },
  {
    "objectID": "Attendance.html",
    "href": "Attendance.html",
    "title": "Packers VS Viking Attendance",
    "section": "",
    "text": "The original data came from this data set: https://github.com/rfordatascience/tidytuesday/blob/main/data/2020/2020-02-04/attendance.csv?plain=1\nThis data compares the attendence of the Green Bay packers and the Minnesota Vikings throughout the seasons of 2018 and 2019. Based on the graphs, it appears that on average the Packers typically have a higher attendence during a game compared to the Vikings. There are fluctuations however due to things such as home and aways games and success that season. For example the Packers had higher success in the 2019 season than in 2018 so their attendence was higher throught the year."
  },
  {
    "objectID": "Project 4.html",
    "href": "Project 4.html",
    "title": "Review of Amazon AI Ethics",
    "section": "",
    "text": "Both of the articles that I selected dove into issues surrounding advancements in AI and its use around hiring. The scenario these articles focus on is how an AI developed by Amazon used to recommend job candidates had to be scrapped based on a bias found present where the AI would favor recommending men and penalized applicants whose resume contained the word woman such as woman’s chess club (Reuters article). This is due to the data that was collected by Amazon, which contained an inherent bias already. These articles both layed out those facts very similarly. Where they differed was how this issue arose. Reuters argued that the blame was not on Amazon and they handled the situation as best as they could. They argued that it was difficult to see a bias like this coming and that it would be difficult to predict this outcome. The ACLU on the other hand argued something different. They claimed that it is in fact on Amazon. They stated that it is a structural injustice embedded in technology and that it is up to the creators of technologies, such as the AI amazon made, to ensure that these biases don’t appear. Where Reuters’ argument fell behind was the fact that Amazon was unable to deny the fact that they had used the AI in consideration before it was scrapped. That means this bias was being used, something that is illegal, though hard to prove in the court of law (ACLU).\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations? The data collection process was an internal process. Amazon used their size and history to get data on all their hires. They then trained their AI on that data set. They claimed that this is a good way of collecting data because it would theoretically get applicants that followed the same way of thinking and work ethic as others they hired before. What actually happened was that since the tech world is majority male, the AI was trained to hire a majority of men and started to penalize women applicants. This shows that the observations collected were probably not ethical as it led to a biased result. Since the data was their own private data, we are unsure if there are any missing observations.\nWere the data made publicly available? The data was not publicly available because they used private hiring data internally but we do know that the data they used was majority male which led to the bias that they found when trying to use this technology. Something both articles stressed was that having a hiring process that is hidden behind doors makes it really hard to tell if this bias affected your resume. On top of this, because they didn’t make the data available, probably due to the safety of their previous employees, it is difficult to actually know the specific cause of the failed AI.\nIs the data being used in unintended ways to the original study? In this scenario, the data was intended to allow Amazon to speed up the hiring process and allow them to spend less energy on the initial part of recruiting. This data was used for that, however, since the data contained a bias from the beginning, the actual result was not what was intended. It actually weeded out resumes that contained words such as women and therefore created a bias against them. The unfortunate thing is that they used the AI to help aid in hiring employees already, therefore they did use it in a way that was unintended, leading to a high likelihood that the bias towards men grew following its use.\nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender? Race should not be used as a variable. What occurs when race is a variable, such as how gender was used in this scenario, is that any bias that was previously occurring due to human bias would lead to affect the data set which would be used by an algorithm or AI. In my example, gender is often discriminated against and there are also often not many women in tech, therefore the data that the AI was trained on thought that being a woman was bad for hiring so the AI learned to not recommend women nearly as often. This led to the issue we are currently in. Additionally, in other scenarios such as algorithms within prisons, by including race, when the algorithm is trained on past data the racism that was used in the past will show up in the algorithm leading to continued racism.\nCitations:\nAmazon hiring algorithm (systemic bias and accountability) Goodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women,” ACLU.\nAmazon hiring algorithm (who has moral responsibility) Dastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women, Reuters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SQL Connection.html",
    "href": "SQL Connection.html",
    "title": "SQL Connection",
    "section": "",
    "text": "library(DBI)\nlibrary(dplyr)\nlibrary(tidyverse)\n\nFor this project we were given access to the database that contains traffic stops throughout the US. As someone that grew up in Oregon and now goes to school in California, I was curious about the three west coast states. I wanted to look at the differences in racial breakdowns of the stops between the three states. Furthermore I was curious how this breakdown compares to the true population breakdown of these states.\nFirst we connected to the server using code that is not shown so I don’t show the password.\nNext we wanted to look at the breakdowns of the traffic stops depending on race. To look at this we looked at each states traffic stop data and combined the data together into percentages of the states traffic stops. This then allowed me to get a table with all three states and their ethnic breakdown.\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\n\nwest_coast_sql\n\n             subject_race count_race percentage      state\n1                   white   14077645   44.29925 California\n2                hispanic   10512845   33.08161 California\n3                   black    2617665    8.23722 California\n4                   other    2390504    7.52239 California\n5  asian/pacific islander    2179855    6.85952 California\n6                 unknown          1    0.00000 California\n7                   white     983532   86.04701     Oregon\n8                hispanic     105491    9.22917     Oregon\n9  asian/pacific islander      24089    2.10749     Oregon\n10                  black      20957    1.83348     Oregon\n11                  other       5080    0.44444     Oregon\n12                unknown       3868    0.33840     Oregon\n13                  white    6287746   55.47966 Washington\n14                unknown    3193337   28.17628 Washington\n15               hispanic     794563    7.01079 Washington\n16 asian/pacific islander     511806    4.51590 Washington\n17                  black     403349    3.55893 Washington\n18                  other     142624    1.25844 Washington\n\n\nNext I graphed all three of the states in a side by side plot to compare the differences directly. What we found was that Oregon had far and away the most people that were considered white at the traffic stops. While California had the lowest number of white people, we saw they had the highest hispanic make-up, something to be expected as their state boarders Mexico. The most interesting finding from this was that a high quantity of the stops were unkown in Washington. This leads me to believe that they either arn’t able to ask their race/ethnicity when they make a stop or they struggle to ask consistently.\n\nggplot(west_coast_sql, aes(x = subject_race, y = percentage, fill = state)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Percentage of Each Race by State\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = \"State\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext I was curious not about the differences between the states but the differences between the traffic stop breakdown and the true population makeup of these states. To figure this out I found data from the census that I was able to build into the table that we originally made. This allowed me to set up a chart where we could compare the differences.\n\n-- Static population data\nSELECT \n  subject_race, \n  NULL AS count_race, \n  percentage, \n  state\nFROM (\n  SELECT 'white' AS subject_race, 71.0 AS percentage, 'Oregon Population' AS state\n  UNION ALL SELECT 'hispanic', 15.16, 'Oregon Population'\n  UNION ALL SELECT 'asian', 4.4, 'Oregon Population'\n  UNION ALL SELECT 'black', 1.8, 'Oregon Population'\n  UNION ALL SELECT 'native american', 1.0, 'Oregon Population'\n  UNION ALL SELECT 'pacific islander', 0.4, 'Oregon Population'\n  UNION ALL SELECT 'two or more races', 6.2, 'Oregon Population'\n\n  UNION ALL SELECT 'white', 63.0, 'Washington Population'\n  UNION ALL SELECT 'hispanic', 14.6, 'Washington Population'\n  UNION ALL SELECT 'asian', 9.3, 'Washington Population'\n  UNION ALL SELECT 'black', 3.8, 'Washington Population'\n  UNION ALL SELECT 'native american', 1.1, 'Washington Population'\n  UNION ALL SELECT 'pacific islander', 0.7, 'Washington Population'\n  UNION ALL SELECT 'two or more races', 7.5, 'Washington Population'\n\n  UNION ALL SELECT 'white', 35.0, 'California Population'\n  UNION ALL SELECT 'hispanic', 39.4, 'California Population'\n  UNION ALL SELECT 'asian', 15.0, 'California Population'\n  UNION ALL SELECT 'black', 5.0, 'California Population'\n  UNION ALL SELECT 'native american', 0.8, 'California Population'\n  UNION ALL SELECT 'pacific islander', 0.3, 'California Population'\n  UNION ALL SELECT 'two or more races', 4.5, 'California Population'\n) AS static_data\n\nUNION ALL\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\nOnce we had the table built, we compared them by faceting them side by side with the traffic stops on the left and the true populations on the right. This allowed for easy comparison. What we found in California was that more of the people involved in traffic stops were white than in the true population. The difference showed up in the hipanic part of the graph with less hispanic stopping compared to the true population. This could be because the true population is self reported and the police making the stop struggling to know their true ethnicity. We saw a similar thing occur in Oregon as well. The differences there were probably due to the self reporting once again. The only state that had an increase in white from traffic stops to the actual population was Washington. Going against the other two, this leads us to question if it is due to the high number of unknown or if it is due to a way the traffic stops are being made.\n\nggplot(combined_data, aes(x = subject_race, y = percentage, fill = subject_race)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ state, nrow = 3, ncol = 2) +\n  labs(\n    title = \"Original Racial Composition by State (from SQL Data)\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 12), \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14)\n  ) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nOverall I wanted to look at the three states and the way race is compared internally and within the three states. What I found was that there was poor data collection when the traffic stops were made but there is a high likely hood that these stops are not made with an underlying bias if the traffic stops were made with the errors there appears to be. That being said there needs to be further investigation into the way data is collected to make sure that we are able to get accurate data that reflects what is put on the census forms.\n\nDBI::dbDisconnect(con_traffic)\n\nRefrences\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nU.S. Census Bureau. 2023. QuickFacts: California; Oregon; Washington. U.S. Department of Commerce. Retrieved from https://www.census.gov/quickfacts/"
  },
  {
    "objectID": "SQL_Connection.html",
    "href": "SQL_Connection.html",
    "title": "SQL Connection",
    "section": "",
    "text": "library(DBI)\nlibrary(dplyr)\nlibrary(tidyverse)\n\nFor this project we were given access to the database that contains traffic stops throughout the US. As someone that grew up in Oregon and now goes to school in California, I was curious about the three west coast states. I wanted to look at the differences in racial breakdowns of the stops between the three states. Furthermore I was curious how this breakdown compares to the true population breakdown of these states.\nFirst we connected to the server using code that is not shown so I don’t show the password.\nNext we wanted to look at the breakdowns of the traffic stops depending on race. To look at this we looked at each states traffic stop data and combined the data together into percentages of the states traffic stops. This then allowed me to get a table with all three states and their ethnic breakdown.\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\n\nwest_coast_sql\n\n             subject_race count_race percentage      state\n1                   white   14077645   44.29925 California\n2                hispanic   10512845   33.08161 California\n3                   black    2617665    8.23722 California\n4                   other    2390504    7.52239 California\n5  asian/pacific islander    2179855    6.85952 California\n6                 unknown          1    0.00000 California\n7                   white     983532   86.04701     Oregon\n8                hispanic     105491    9.22917     Oregon\n9  asian/pacific islander      24089    2.10749     Oregon\n10                  black      20957    1.83348     Oregon\n11                  other       5080    0.44444     Oregon\n12                unknown       3868    0.33840     Oregon\n13                  white    6287746   55.47966 Washington\n14                unknown    3193337   28.17628 Washington\n15               hispanic     794563    7.01079 Washington\n16 asian/pacific islander     511806    4.51590 Washington\n17                  black     403349    3.55893 Washington\n18                  other     142624    1.25844 Washington\n\n\nNext I graphed all three of the states in a side by side plot to compare the differences directly. What we found was that Oregon had far and away the most people that were considered white at the traffic stops. While California had the lowest number of white people, we saw they had the highest hispanic make-up, something to be expected as their state boarders Mexico. The most interesting finding from this was that a high quantity of the stops were unkown in Washington. This leads me to believe that they either arn’t able to ask their race/ethnicity when they make a stop or they struggle to ask consistently.\n\nggplot(west_coast_sql, aes(x = subject_race, y = percentage, fill = state)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Percentage of Each Race by State\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = \"State\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNext I was curious not about the differences between the states but the differences between the traffic stop breakdown and the true population makeup of these states. To figure this out I found data from the census that I was able to build into the table that we originally made. This allowed me to set up a chart where we could compare the differences.\n\n-- Static population data\nSELECT \n  subject_race, \n  NULL AS count_race, \n  percentage, \n  state\nFROM (\n  SELECT 'white' AS subject_race, 71.0 AS percentage, 'Oregon Population' AS state\n  UNION ALL SELECT 'hispanic', 15.16, 'Oregon Population'\n  UNION ALL SELECT 'asian', 4.4, 'Oregon Population'\n  UNION ALL SELECT 'black', 1.8, 'Oregon Population'\n  UNION ALL SELECT 'native american', 1.0, 'Oregon Population'\n  UNION ALL SELECT 'pacific islander', 0.4, 'Oregon Population'\n  UNION ALL SELECT 'two or more races', 6.2, 'Oregon Population'\n\n  UNION ALL SELECT 'white', 63.0, 'Washington Population'\n  UNION ALL SELECT 'hispanic', 14.6, 'Washington Population'\n  UNION ALL SELECT 'asian', 9.3, 'Washington Population'\n  UNION ALL SELECT 'black', 3.8, 'Washington Population'\n  UNION ALL SELECT 'native american', 1.1, 'Washington Population'\n  UNION ALL SELECT 'pacific islander', 0.7, 'Washington Population'\n  UNION ALL SELECT 'two or more races', 7.5, 'Washington Population'\n\n  UNION ALL SELECT 'white', 35.0, 'California Population'\n  UNION ALL SELECT 'hispanic', 39.4, 'California Population'\n  UNION ALL SELECT 'asian', 15.0, 'California Population'\n  UNION ALL SELECT 'black', 5.0, 'California Population'\n  UNION ALL SELECT 'native american', 0.8, 'California Population'\n  UNION ALL SELECT 'pacific islander', 0.3, 'California Population'\n  UNION ALL SELECT 'two or more races', 4.5, 'California Population'\n) AS static_data\n\nUNION ALL\n\n-- Oregon arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM or_statewide_2020_04_01) AS percentage,\n  'Oregon' AS state\nFROM or_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- Washington arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM wa_statewide_2020_04_01) AS percentage,\n  'Washington' AS state\nFROM wa_statewide_2020_04_01\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nUNION ALL\n\n-- California arrest data\nSELECT\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END AS subject_race,\n  COUNT(*) AS count_race,\n  100.0 * COUNT(*) / (SELECT COUNT(*) FROM ca_statewide_2023_01_26) AS percentage,\n  'California' AS state\nFROM ca_statewide_2023_01_26\nGROUP BY\n  CASE WHEN subject_race IS NULL THEN 'unknown' ELSE subject_race END\n\nORDER BY\n  state,\n  percentage DESC;\n\nOnce we had the table built, we compared them by faceting them side by side with the traffic stops on the left and the true populations on the right. This allowed for easy comparison. What we found in California was that more of the people involved in traffic stops were white than in the true population. The difference showed up in the hipanic part of the graph with less hispanic stopping compared to the true population. This could be because the true population is self reported and the police making the stop struggling to know their true ethnicity. We saw a similar thing occur in Oregon as well. The differences there were probably due to the self reporting once again. The only state that had an increase in white from traffic stops to the actual population was Washington. Going against the other two, this leads us to question if it is due to the high number of unknown or if it is due to a way the traffic stops are being made.\n\nggplot(combined_data, aes(x = subject_race, y = percentage, fill = subject_race)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ state, nrow = 3, ncol = 2) +\n  labs(\n    title = \"Original Racial Composition by State (from SQL Data)\",\n    x = \"Race\",\n    y = \"Percentage\",\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 12), \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14)\n  ) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nOverall I wanted to look at the three states and the way race is compared internally and within the three states. What I found was that there was poor data collection when the traffic stops were made but there is a high likely hood that these stops are not made with an underlying bias if the traffic stops were made with the errors there appears to be. That being said there needs to be further investigation into the way data is collected to make sure that we are able to get accurate data that reflects what is put on the census forms.\n\nDBI::dbDisconnect(con_traffic)\n\nRefrences\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nU.S. Census Bureau. 2023. QuickFacts: California; Oregon; Washington. U.S. Department of Commerce. Retrieved from https://www.census.gov/quickfacts/"
  }
]