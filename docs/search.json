[
  {
    "objectID": "canadian_births.html",
    "href": "canadian_births.html",
    "title": "Canadian Births",
    "section": "",
    "text": "The original data came from this data set: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-01-09/canada_births_1991_2022.csv\nThis data shows how births varied in Canada throughout an individual year and over the years from 1990 to 2022. Over the 32 years, we see a dip until 2020 before it rises back up again until it peaks at 2015. It appears that the birth rate has a rising and falling pattern shifting from rising to falling every 10-15 years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I am a student at Pomona College. I swim on the swim team and plan to major in either Computer Science, or Math. In my free time I like to hang out with friends, kiteboard/wingfoil, and play games."
  },
  {
    "objectID": "TheOfficeLineAnalysis.html",
    "href": "TheOfficeLineAnalysis.html",
    "title": "The Office Line-Analysis",
    "section": "",
    "text": "For this project, I chose to analyze a data set containing every line from the she “The Office”. The data set was found at https://www.kaggle.com/datasets/fabriziocominetti/the-office-lines?resource=download. In this data set I wanted to look at how the characters from my favorite show appeared through the show and how an iconic joke appeared as well.\n\n\nShow the code\ntop_characters &lt;- the_office_lines |&gt;\n  group_by(Character) |&gt;\n  summarize(total_count = n(), .groups = \"drop\") |&gt;\n  slice_max(total_count, n = 10)\n\nlines_per_season &lt;- the_office_lines |&gt;\n  filter(Character %in% top_characters$Character) |&gt;\n  group_by(Character, Season, add = TRUE) |&gt;\n  summarize(appearences = n(), .groups = \"drop\") |&gt;\n  group_by(Season) |&gt;\n  mutate(total_season = sum(appearences)) |&gt;\n  ungroup() |&gt;\n  mutate(percent = (appearences/total_season)*100)\n\nggplot(lines_per_season, aes(x = Season, y = percent, color = Character)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  labs(\n    x = \"Season\",\n    y = \"Percentage\",\n    title = \"Line Breakdown Per Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe first thing that I analyzed is in the graph above, showing the distribution of lines characters have each season of the show. The graph shows the 10 characters with the highest total lines in the show and their share of each season of the office between the 10. As you can see Micheal dominates the share of lines through his time with the office, only slightly declining throughout. As he leaves the show following season 8, you see a jump in other characters roles as they needed to fill the gap that Michael left, such as Andy and Dwight. This graph shows the importance that Michael had to the show and how much it changed in its last two seasons.\n\n\nShow the code\nthats_what_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_lines = str_to_lower(Line)) |&gt;\n  mutate(quote = str_extract(lower_lines, \"that’s what she said\")) |&gt;\n  filter(!is.na(quote)) |&gt;\n  mutate(season_text = str_c(Season)) |&gt;\n  group_by(season_text) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  add_row(count = 0, season_text = \"1\") |&gt;\n  add_row(count = 0, season_text = \"8\")\n\nggplot(thats_what_she_said, aes(x = season_text, y = count)) +\n  geom_col() +\n  labs(\n    x = \"Season\",\n    y = \"Quantity of Jokes\",\n    title = \"That's What She Said Jokes a Season\"\n  ) +\n  theme(\n    axis.text.x = element_text(size = 14),\n    axis.text.y = element_text(size = 14), \n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe next thing I looked at focused on Michaels famous joke, “that’s what she said”. The chart above breaks down the usage in the show throughout the nine seasons. As you can see in the chart, they created the joke starting in season 2 and slowly declined in use as the show went on before Michael’s departure following season season 7. There is a return in season 9, likely due to Michael’s return for the finale.\n\n\nShow the code\nwords_preceding_she_said &lt;- the_office_lines |&gt;\n  mutate(lower_case = str_to_lower(Line)) |&gt;\n  mutate(char_following = str_extract(lower_case, \"(?&lt;=that’s what she said).\")) |&gt;\n  filter(!is.na(char_following)) |&gt;\n  group_by(char_following) |&gt;\n  summarize(count = n())\n  \nggplot(words_preceding_she_said, aes(x = char_following, y = count, text = char_following)) +\n  geom_col() +\n  labs(\n    x = \"Character following\",\n    y = \"Appearences\",\n    title = 'Character following \"Thats what she said\"'\n  ) +\n  theme(\n    axis.text.x = element_text(size = 20, face = \"bold\"),\n    axis.text.y = element_text(size = 14),\n    axis.title.x = element_text(size = 16),\n    axis.title.y = element_text(size = 16),\n    plot.title = element_text(size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nFinally I was curious how this joke was typically written into the show. The chart above does this by breaking down which punctuation was used following the joke. This is able to tell us how the joke was presented. For example, we are able to figure out that more often than not, the joke is not written as Michael screaming out “That’s what she said!”, rather he delivers it less excited. You are also able to see that it is written in other ways as well such as the ’, which is a line where Jim quotes the joke inside of a question. What we see is they often didn’t direct the joke to be delivered with a lot of excitement, rather they left it up to the actor who was making the joke to decide how to deliver it."
  },
  {
    "objectID": "Game_Simulations.html",
    "href": "Game_Simulations.html",
    "title": "Basketball Game Simulations",
    "section": "",
    "text": "For this project I amied to simulate basketball games based on both the teams strength and on the opposing teams strength. First I will build a function that can simulate a basketball game based on two inputs, the strength of the focus team and the strength of the opposing team. To create variability we will make opposing strength more a random strength value centered around the input. Next we will simulate this many times to get a season percentage. We will then repeat the season percentage for each pairing of strengths 1-10 many times to get the statistics.\nThe following code is the simulation done for each game. Here is an example output:\n\n\nShow the code\nsimulate_game &lt;- function(count, strength, opposing_strength_avg) {\n  opposing_strength &lt;- rnorm(1, opposing_strength_avg, 1)\n  \n  main_team_score = round(rnorm(1, mean = 115 + strength - opposing_strength, abs(1-strength)))\n  away_team_score = round(rnorm(1, mean = 115 + opposing_strength - strength, abs(1-strength)))\n  difference = main_team_score - away_team_score\n  if (difference == 0) {\n    if (strength &gt; opposing_strength) {\n      main_team_score &lt;- main_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    } else if (strength &lt; opposing_strength) {\n      away_team_score &lt;- away_team_score + 1\n      difference &lt;- main_team_score - away_team_score\n    }\n  }\n  \n  results &lt;- data.frame(\n    game = count,\n    main_team_score = main_team_score,\n    away_team_score = away_team_score,\n    strength = strength,\n    opposing_strength = opposing_strength,\n    difference = difference\n  )\n  return (results)\n}\n\nsimulate_game(1, 5, 5)\n\n\n  game main_team_score away_team_score strength opposing_strength difference\n1    1             116             118        5          6.994696         -2\n\n\nThis next code chunk simulates the previous game simulation 82 times, giving a season percentage. Here is an example output for that code:\n\n\nShow the code\nseason_simulation &lt;- function(count, strength_input,  opposing_strength_avg_input) {\n  params &lt;- list(\n    count = 1:82,\n    strength = rep(strength_input, 82),\n    opposing_strength_avg = rep(opposing_strength_avg_input, 82)\n  )\n\n  season &lt;- list_rbind(pmap(params, simulate_game)) |&gt;\n    select(difference) |&gt;\n    mutate(win = difference &gt; 0)\n  return_df &lt;- data.frame(\n    season_perc = mean(season$win)\n  )\n  return (return_df)\n}\n\nseason_simulation(1, 5, 5)\n\n\n  season_perc\n1   0.5365854\n\n\nNext this code runs the previous simulation 50 times giving us 50 seasons of 1 strength input combination allowing us to use Central Limit Theorem on the outputted values and giving us reliable data:\n\n\nShow the code\nperc_sim &lt;- function(strength, opposing_strength) {\n  params &lt;- list(\n      count = 1:50,\n      strength_input = rep(strength, 50),\n      opposing_strength_avg_input = rep(opposing_strength, 50)\n    )\n  itterations &lt;- list_rbind(pmap(params, season_simulation)) |&gt;\n    mutate(strength_level = strength) |&gt;\n    mutate(opposing_strength = opposing_strength) |&gt;\n    mutate(winning_perc = mean(season_perc))\n  return(\n    itterations |&gt;\n      summarise(\n        winning_perc = mean(winning_perc),\n        strength_level = first(strength_level),\n        opposing_strength = first(opposing_strength)\n      )\n  )\n}\n\nperc_sim(5, 5)\n\n\n  winning_perc strength_level opposing_strength\n1    0.5004878              5                 5\n\n\nFinally to finish off our simulation I am going to run the previous code chunk for each combination of code giving us a data frame that has an average percentage for each combination of strength inputs.\n\n\nShow the code\nsim_params &lt;- list(\n  strength = rep(1:10, times = 10),\n  opposing_strength = rep(1:10, each = 10)\n)\n\nperc_breakdown &lt;- list_rbind(pmap(sim_params, perc_sim))\n\n\nNow to visualize the data, the first graph is a heat map with the focus teams strength on the x-axis and the opposing strength team on the y-axis. This gives us a good showing of how the matchups go. Interestingly, there is a slight advantage to the focus team when they are roughly equal but both have high input values compared to low.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level, y = opposing_strength, fill = winning_perc)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"red\", high = \"green\", name = \"Win %\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe other visual representation we have is a scatter plot that has strength - opposing strength on the x-axis and win percentage on the y-axis. This shows an interesting layout of how fast the differences in strength can affect the results of the games.\n\n\nShow the code\nggplot(perc_breakdown, aes(x = strength_level - opposing_strength, y = winning_perc)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThese graphs show great insight into just how important it is to have a strong team relative to the rest of the league. Despite the simple simulation for each game, we find that just by having a somewhat equal team relative to the rest of the league you will make the playoffs, the average wins of the worst seed is around 50%. We also see that it only takes around a 2 or 3 strength index advantage to be able to get a very strong season put together. That being said this shows just how even the teams are in strength. The next steps would be to use real data and team analysis to be able to pinpoint just what the strngth index would be based off of but this is a very interesting and good start to this type of analysis."
  },
  {
    "objectID": "Attendance.html",
    "href": "Attendance.html",
    "title": "Packers VS Viking Attendance",
    "section": "",
    "text": "The original data came from this data set: https://github.com/rfordatascience/tidytuesday/blob/main/data/2020/2020-02-04/attendance.csv?plain=1\nThis data compares the attendence of the Green Bay packers and the Minnesota Vikings throughout the seasons of 2018 and 2019. Based on the graphs, it appears that on average the Packers typically have a higher attendence during a game compared to the Vikings. There are fluctuations however due to things such as home and aways games and success that season. For example the Packers had higher success in the 2019 season than in 2018 so their attendence was higher throught the year."
  },
  {
    "objectID": "Project 4.html",
    "href": "Project 4.html",
    "title": "Review of Amazon AI Ethics",
    "section": "",
    "text": "Both of the articles that I selected dove into issues surrounding advancements in AI and its use around hiring. The scenario these articles focus on is how an AI developed by Amazon used to recommend job candidates had to be scrapped based on a bias found present where the AI would favor recommending men and penalized applicants whose resume contained the word woman such as woman’s chess club (Reuters article). This is due to the data that was collected by Amazon, which contained an inherent bias already. These articles both layed out those facts very similarly. Where they differed was how this issue arose. Reuters argued that the blame was not on Amazon and they handled the situation as best as they could. They argued that it was difficult to see a bias like this coming and that it would be difficult to predict this outcome. The ACLU on the other hand argued something different. They claimed that it is in fact on Amazon. They stated that it is a structural injustice embedded in technology and that it is up to the creators of technologies, such as the AI amazon made, to ensure that these biases don’t appear. Where Reuters’ argument fell behind was the fact that Amazon was unable to deny the fact that they had used the AI in consideration before it was scrapped. That means this bias was being used, something that is illegal, though hard to prove in the court of law (ACLU).\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations? The data collection process was an internal process. Amazon used their size and history to get data on all their hires. They then trained their AI on that data set. They claimed that this is a good way of collecting data because it would theoretically get applicants that followed the same way of thinking and work ethic as others they hired before. What actually happened was that since the tech world is majority male, the AI was trained to hire a majority of men and started to penalize women applicants. This shows that the observations collected were probably not ethical as it led to a biased result. Since the data was their own private data, we are unsure if there are any missing observations.\nWere the data made publicly available? The data was not publicly available because they used private hiring data internally but we do know that the data they used was majority male which led to the bias that they found when trying to use this technology. Something both articles stressed was that having a hiring process that is hidden behind doors makes it really hard to tell if this bias affected your resume. On top of this, because they didn’t make the data available, probably due to the safety of their previous employees, it is difficult to actually know the specific cause of the failed AI.\nIs the data being used in unintended ways to the original study? In this scenario, the data was intended to allow Amazon to speed up the hiring process and allow them to spend less energy on the initial part of recruiting. This data was used for that, however, since the data contained a bias from the beginning, the actual result was not what was intended. It actually weeded out resumes that contained words such as women and therefore created a bias against them. The unfortunate thing is that they used the AI to help aid in hiring employees already, therefore they did use it in a way that was unintended, leading to a high likelihood that the bias towards men grew following its use.\nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender? Race should not be used as a variable. What occurs when race is a variable, such as how gender was used in this scenario, is that any bias that was previously occurring due to human bias would lead to affect the data set which would be used by an algorithm or AI. In my example, gender is often discriminated against and there are also often not many women in tech, therefore the data that the AI was trained on thought that being a woman was bad for hiring so the AI learned to not recommend women nearly as often. This led to the issue we are currently in. Additionally, in other scenarios such as algorithms within prisons, by including race, when the algorithm is trained on past data the racism that was used in the past will show up in the algorithm leading to continued racism.\nCitations:\nAmazon hiring algorithm (systemic bias and accountability) Goodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women,” ACLU.\nAmazon hiring algorithm (who has moral responsibility) Dastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women, Reuters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]