---
title: "Review of Amazon AI Ethics"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
knitr:
  opts_chunk:
    warning: false
---

Both of the articles that I selected dove into issues surrounding advancements in AI and its use around hiring. The scenario these articles focus on is how an AI developed by Amazon used to recommend job candidates had to be scrapped based on a bias found present where the AI would favor recommending men and penalized applicants whose resume contained the word woman such as woman’s chess club (Reuters article). This is due to the data that was collected by Amazon, which contained an inherent bias already. These articles both layed out those facts very similarly. Where they differed was how this issue arose. Reuters argued that the blame was not on Amazon and they handled the situation as best as they could. They argued that it was difficult to see a bias like this coming and that it would be difficult to predict this outcome. The ACLU on the other hand argued something different. They claimed that it is in fact on Amazon. They stated that it is a structural injustice embedded in technology and that it is up to the creators of technologies, such as the AI amazon made, to ensure that these biases don’t appear. Where Reuters' argument fell behind was the fact that Amazon was unable to deny the fact that they had used the AI in consideration before it was scrapped. That means this bias was being used, something that is illegal, though hard to prove in the court of law (ACLU).

**What was the data collection process? Were the observations collected ethically? Are there missing observations?** The data collection process was an internal process. Amazon used their size and history to get data on all their hires. They then trained their AI on that data set. They claimed that this is a good way of collecting data because it would theoretically get applicants that followed the same way of thinking and work ethic as others they hired before. What actually happened was that since the tech world is majority male, the AI was trained to hire a majority of men and started to penalize women applicants. This shows that the observations collected were probably not ethical as it led to a biased result. Since the data was their own private data, we are unsure if there are any missing observations.

**Were the data made publicly available?** The data was not publicly available because they used private hiring data internally but we do know that the data they used was majority male which led to the bias that they found when trying to use this technology. Something both articles stressed was that having a hiring process that is hidden behind doors makes it really hard to tell if this bias affected your resume. On top of this, because they didn’t make the data available, probably due to the safety of their previous employees, it is difficult to actually know the specific cause of the failed AI.

**Is the data being used in unintended ways to the original study?** In this scenario, the data was intended to allow Amazon to speed up the hiring process and allow them to spend less energy on the initial part of recruiting. This data was used for that, however, since the data contained a bias from the beginning, the actual result was not what was intended. It actually weeded out resumes that contained words such as women and therefore created a bias against them. The unfortunate thing is that they used the AI to help aid in hiring employees already, therefore they did use it in a way that was unintended, leading to a high likelihood that the bias towards men grew following its use.

**Should race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?** Race should not be used as a variable. What occurs when race is a variable, such as how gender was used in this scenario, is that any bias that was previously occurring due to human bias would lead to affect the data set which would be used by an algorithm or AI. In my example, gender is often discriminated against and there are also often not many women in tech, therefore the data that the AI was trained on thought that being a woman was bad for hiring so the AI learned to not recommend women nearly as often. This led to the issue we are currently in. Additionally, in other scenarios such as algorithms within prisons, by including race, when the algorithm is trained on past data the racism that was used in the past will show up in the algorithm leading to continued racism.

**Citations:**

Amazon hiring algorithm (systemic bias and accountability) Goodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women,” ACLU.

Amazon hiring algorithm (who has moral responsibility) Dastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women, Reuters.
