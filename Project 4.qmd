---
title: "Review of Amazon AI Ethics"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
knitr:
  opts_chunk:
    warning: false
---

Both of the articles that I selected dove into issues surrounding advancements in AI and its use around hiring. The scenario these articles focus on is how an AI developed by Amazon, used to recommend job candidates, had to be scrapped based on a bias found present where the AI would favor recommending men and penalized applicants whose resume contained the word woman such as woman’s chess club (Dastin). This is due to the data that was collected by Amazon, which contained an inherent bias already. These articles both layed out those facts very similarly. Where they differed was how this issue arose. Reuters argued that the blame was not on Amazon and they handled the situation as best as they could. They argued that it was difficult to see a bias like this coming and that it would be difficult to predict this outcome. The ACLU on the other hand argued something different. They claimed that it is in fact on Amazon. They stated that it is a structural injustice embedded in technology and that it is up to the creators of technologies, such as the AI amazon made, to ensure that these biases don’t appear. Where Reuters' argument fell behind was the fact that Amazon was unable to deny the fact that they had used the AI in consideration before it was scrapped. That means this bias was being used, something that is illegal, though hard to prove in the court of law (Goodman).

**What was the data collection process? Were the observations collected ethically? Are there missing observations?** Amazon’s data collection process was entirely internal, drawing on its own hiring history to train the AI. The company argued that using this data would help identify applicants who shared traits—such as mindset or work ethic—with previously successful hires. However, because the tech industry has historically been male-dominated, the data itself was skewed. As a result, the AI began to favor male candidates and penalize resumes associated with women. This outcome highlights a key ethical concern: the observations used for training were inherently biased, and relying on them reinforced discriminatory patterns. Since the data was proprietary and not publicly released, it’s also unclear whether there were missing or unbalanced observations that further affected the algorithm’s performance.

**Were the data made publicly available?** The data used in Amazon’s hiring algorithm was not publicly available, as it came from internal recruitment records. However, it is known that the data reflected a predominantly male applicant pool, which contributed to the gender bias observed in the AI's recommendations. Both articles emphasized that when hiring processes are

**Is the data being used in unintended ways to the original study?** In this scenario, Amazon intended to use data and AI to streamline the hiring process and reduce the effort spent on early-stage recruitment. While the system achieved that goal technically, it did so by learning from biased historical data. Because past hiring patterns reflected gender bias, the AI began to penalize resumes containing words like “women,” leading to discriminatory outcomes. Although the original intent was efficiency, the AI ended up replicating and amplifying bias, particularly against women. What makes this especially concerning is that Amazon actively used the AI in its hiring process, meaning it likely influenced real decisions and may have increased gender bias in hiring rather than reducing it.

**Should race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?** Race should not be used as a variable in algorithms. When race (or gender) is included as a variable, any preexisting human biases embedded in historical data will be learned and amplified by the algorithm. Additionally, it will just increase the rate that these biases are done. For example, in a hiring scenario, if past decisions reflect gender discrimination—such as underrepresentation of women in tech roles—an AI trained on that data may learn to associate being a woman with lower hiring potential, resulting in discriminatory outcomes. Similarly, in criminal justice systems, including race as a variable can cause the algorithm to replicate and reinforce past racial biases, such as those present in arrest or sentencing records. In both cases, using sensitive identity markers like race or gender risks perpetuating structural discrimination rather than correcting it.

**Citations:**

Amazon hiring algorithm (systemic bias and accountability) Goodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women,” ACLU.

Amazon hiring algorithm (who has moral responsibility) Dastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women, Reuters.
